# Spark configuration
spark:
  app_name: "ECommerce Data Processing"
  master: "local[*]"
  log_level: "WARN"

# Batch processing configuration
batch:
  input_path: "data/batch/"
  output_path: "data/warehouse/batch/"
  quarantine_path: "data/warehouse/batch/quarantine/"
  cancelled_path: "data/warehouse/batch/cancelled/"

# Stream processing configuration
stream:
  input_path: "data/stream/input/"
  output_path: "data/warehouse/stream/"
  checkpoint_path: "data/warehouse/stream/checkpoint/"
  max_files_per_trigger: 2
  watermark_delay: "30 minutes"
  window_duration: "10 minutes"

# Data generator configuration
generator:
  num_records: 10000
  start_date: "2026-01-01"
  end_date: "2026-01-31"

  num_products: 50
  num_customers: 1000

  # Price range
  min_price: 5.0
  max_price: 500.0

  # Data quality issues (percentages)
  duplicate_rate: 0.02          # 2% duplicates
  negative_quantity_rate: 0.05  # 5% negative quantity
  negative_price_rate: 0.05     # 5% negative price
  late_data_rate: 0.05          # 5% late arrivals
  missing_id_rate: 0.01         # 1% missing ids

  # Output
  output_file: "data/batch/orders_20260101.csv"

# Dimension tables
dimensions:
  customers_path: "data/dimensions/customers.csv"
  products_path: "data/dimensions/products.csv"

# Event generator (for streaming simulation)
event_generator:
  source_file: "data/batch/orders_20260101.csv"
  output_dir: "data/stream/input/"
  batch_size: 100
  sleep_min: 1
  sleep_max: 3